### MotivaÃ§Ã£o e Funcionamento
Baseado nas aulas do [@Badtuxx](https://github.com/Badtuxx) no PICK 2024, resolvi criar esse projeto para realizar a criaÃ§Ã£o de um cluster K8S PARA ESTUDOS de forma automatizada utilizando instÃ¢ncias EC2 na AWS. 
A ideia aqui Ã© ter total controle do cluster, inclusive do control-plane, para que possamos configurar o que quisermos para entender melhor o funcionamento do Kubernetes. Outra vantagem Ã© o custo, pois dependendo da configuraÃ§Ã£o que vocÃª criar este cluster, poderÃ¡ utilizÃ¡-lo com um valor bem baixo, tirando o maior proveito possÃ­vel do Free Tier da AWS. Leia atentamente a seÃ§Ã£o ["***Utilizando o Cluster Gratuitamente***"](https://github.com/FabioBartoli/k8s-with-ec2?tab=readme-ov-file#utilizando-o-cluster-gratuitamente)

Este projeto utiliza o **Terraform** para provisionar o ambiente de forma automatizada, facilitando tanto a criaÃ§Ã£o quanto a exclusÃ£o ou atualizaÃ§Ã£o dos recursos,  e o **Github Actions**, fazendo com que nÃ£o seja necessÃ¡rio o download do projeto e executÃ¡-lo em sua mÃ¡quina local, caso nÃ£o queira.
Ao final da execuÃ§Ã£o do deploy, vocÃª terÃ¡ um cluster criado e um sistema de arquivos com **NFS Server** para poder realizar seus laboratÃ³rios de Kubernetes
##

> AtenÃ§Ã£o! Por conformidade, eu estou criando todos os recursos em
> *us-east-1*. Se por algum motivo quiser utilizar outra regiÃ£o, lembre-se de alterar esse valor onde encontrÃ¡-lo

##
### Requisitos:
#### FaÃ§a um fork deste repositÃ³rio
Fique Ã  vontade para realizar um fork desse projeto e utilizÃ¡-lo. Assim, vocÃª nÃ£o precisarÃ¡ baixar ou instalar qualquer dependÃªncia em sua mÃ¡quina local
##
#### Tenha uma chave SSH
A primeira coisa que vocÃª precisarÃ¡ para esse projeto Ã© de uma chave SSH. VocÃª pode ver [este tutorial](https://docs.github.com/pt/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent) caso nÃ£o saiba como fazer
Para que o Actions funcione, vocÃª precisarÃ¡ salvar suas chaves SSH privada e pÃºblica como secrets do repositÃ³rio github. Os nomes devem ser "**PRIVATE_KEY**" e "**PUBLIC_KEY**", respectivamente
##
#### Crie um usuÃ¡rio no IAM
O prÃ³ximo passo serÃ¡ criar, em sua AWS Account, um usuÃ¡rio que terÃ¡ as permissÃµes para criar os recursos necessÃ¡rios na AWS. Em [policy.json](./docs/examples/policy.json), eu deixei disponÃ­vel a policy que criei na minha conta e que dÃ¡ todos os acessos que serÃ£o necessÃ¡rios
Gere credenciais para este usuÃ¡rio e tambÃ©m as salve como secrets deste repositÃ³rio no github, com os nome de **ACCESS_KEY** e **SECRET_KEY**
Ao final, vocÃª terÃ¡ as seguintes secrets criadas no repositÃ³rio:
![github secrets](./docs/images/github-secrets.png)
##
#### Crie um Bucket S3
Ele serÃ¡ necessÃ¡rio para armazenarmos o state da nossa configuraÃ§Ã£o do Terraform de forma segura. Para mais informaÃ§Ãµes, leia [State | Terraform](https://developer.hashicorp.com/terraform/language/state)
##
#### Configure seu ambiente
No arquivo [main.tf](./main.tf), serÃ¡ necessÃ¡rio que vocÃª faÃ§a algumas alteraÃ§Ãµes para executar em sua conta da AWS, sendo elas:

 - **variable "k8s_subnet_cidr"**: VocÃª deve escolher um CIDR vÃ¡lido em sua VPC e colocar seu valor nesta variÃ¡vel
 - **backend - bucket**: Aqui, substitua o valor atual pelo nome do seu bucket. A key pode deixar como estÃ¡
 - **Dentro do module "k8s_provisioner"**: Substitua o **vpc_id** pelo ID da sua VPC
Fique Ã  vontade para alterar qualquer outra variÃ¡vel que desejar neste arquivo, mas somente as 3 citadas anteriormente sÃ£o necessÃ¡rias para poder executar o pipeline
##
#### Criando ou destruindo o Cluster
Agora que jÃ¡ configuramos o que era necessÃ¡rio, basta utilizarmos o Github Actions para provisionar o nosso cluster. Para isto, Ã© bem simples: Acesse a aba **"Actions"**, selecione a Action **"PICK EC2 K8S PIPELINE"** e entÃ£o, **"Run Workflow"**
No run, vocÃª poderÃ¡ escolher entre criar o seu cluster (aÃ§Ã£o "apply"), ou removÃª-lo completamente (aÃ§Ã£o "destroy").
ApÃ³s escolher a aÃ§Ã£o, basta clicar em **"Run workflow"** e o ambiente comeÃ§arÃ¡ a ser provisionado.
Quando o pipeline finalizar, vocÃª poderÃ¡ pegar os IPs do seu control-plane e dos workers criados para acessÃ¡-los

![run actions](./docs/images/run-actions-steps.png)

![output ips](./docs/images/public-ips.png)
##
#### Acessando o Cluster
VocÃª precisarÃ¡ da chave privada criada anteriormente para acessar o cluster sempre que quiser. Para isso, basta executar o seguinte comando:

    ssh -i "path/da/chave/id_rsa" ubuntu@ip_publico_da_instancia


##
#### Utilizando o Cluster Gratuitamente
Ã‰ possÃ­vel criar e utilizar este cluster de forma gratuita na AWS ou com um valor bem baixo. Para isso, deve-se tomar alguns cuidados, sendo eles:

 1. *Tenha uma conta na AWS que esteja com o **Free Tier** ativo*: Ã‰ importante que a conta onde vocÃª irÃ¡ criar esse cluster ainda esteja dentro do tempo de 12 meses grÃ¡tis, principalmente para a criaÃ§Ã£o das instÃ¢ncias. Para criar esse cluster, vocÃª perceberÃ¡ que eu estou utilizando no [main.tf](./main.tf) instÃ¢ncias do tipo "t2.micro". Elas estÃ£o abaixo do recomendado para rodar o Kubernetes (*requisito mÃ­nimo Ã© 2vCPU e 2GiB RAM*), mas para meus laboratÃ³rios estÃ¡ suprindo bem (e Ã© free ðŸ˜)
 AlÃ©m disso, vocÃª perceberÃ¡ tambÃ©m que estou definindo valores baixo de disco para cada uma das instÃ¢ncias: 14GiB para o Control Plane e 8GiB para cada Worker. Fiz isso para que o cluster fique dentro dos 30GiB oferecidos gratuitamente pela AWS, e provisionei mais espaÃ§o para o Control-Plane pois Ã© nele que encontra-se o **NFS Server**
 Caso queira utilizar uma instÃ¢ncia com o valor mÃ­nimo recomendado pelo Kubernetes, altere no [main.tf](./main.tf) o *instance_type* para outro valor. A instÃ¢ncia mais barata que atende os requisitos mÃ­nimos na regiÃ£o *us-east-1* Ã© a **t3a.small**, que irÃ¡ gerar um custo de **0.0188 USD por hora** de execuÃ§Ã£o 
 2. *Preste atenÃ§Ã£o nos valores do Free Tier*: Para utilizar o Free Tier sem surpresas, precisamos prestar atenÃ§Ã£o principalmente no seguinte: Este cluster estÃ¡ sendo configurado com 3 mÃ¡quinas EC2 do tipo t2.micro e as 3 possuem um IP pÃºblico atrelado. AlÃ©m disso, a soma dos espaÃ§os em disco das 3 mÃ¡quinas correspondem a 30GiB.
 Se olharmos para a [definiÃ§Ã£o de preÃ§os da AWS](https://aws.amazon.com/free/?nc1=h_ls&all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=tier#12monthsfree&awsf.Free%20Tier%20Categories=*all), veremos os seguinte:
 ![aws price](./docs/images/free-tier.png)

Baseado nesse valor de 750 horas para instÃ¢ncias e 750 horas para IPs pÃºblicos, precisamos dividir isso pelo nÃºmero de instÃ¢ncias que teremos em nosso cluster. No caso, como estou criando 3 instÃ¢ncias (1 control plane + 2 workers), eu teria 250 horas grÃ¡tis por mÃ¡quina + IP por mÃªs, o que seriam cerca de 10 dias/ mÃªs
Se vocÃª aumentar o nÃºmero de instÃ¢ncias, os dias do free tier irÃ£o diminuir na mesma proporÃ§Ã£o. Para que vocÃª possa utilizar pelo mÃ¡ximo de tempo possÃ­vel sem gerar custos, lembre-se sempre de realizar o "**destroy**" da sua infraestrutura sempre que nÃ£o tiver utilizando. SÃ³ nÃ£o esqueÃ§a de salvar seus arquivos que estÃ£o no cluster :o
3. *Por Ãºltimo: crie um alarme de custos na AWS*: Apesar de todo o cuidado na criaÃ§Ã£o do Cluster, algo nÃ£o previsto pode acabar gerando custos em sua conta. Para evitar isso, siga [este tutorial da AWS](https://docs.aws.amazon.com/pt_br/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html) e crie um alarme para ser avisado sempre que sua conta gerar algum custo. Aqui, eu deixei o alarme para avisar quando o custo estimado chegar em 2 dÃ³lares ðŸ’¸

##
### ConsideraÃ§Ãµes finais
Melhorias no projeto sÃ£o bem-vindas, basta mandar um pull request! ðŸ˜€
Como este projeto Ã© para um estudo mais simples, nÃ£o implementei nenhuma lÃ³gica para aumentar o nÃºmero de Control Planes e realizar a configuraÃ§Ã£o de algum tipo de HA quando forem criadas mais mÃ¡quinas. EntÃ£o, independente do nÃºmero que vocÃª passar, serÃ¡ criado apenas 1 Control Plane e o restante serÃ£o workers. Caso queira implementar algo nesse sentido, fique Ã  vontade!
##### Qualquer dÃºvida ou sugestÃ£o, estou Ã  disposiÃ§Ã£o! >> [Telegram](https://t.me/FabioBartoli) <<
